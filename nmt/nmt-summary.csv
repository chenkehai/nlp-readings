year,date,proc,tag,data,title,authors,place,link,description
2013,1310,EMNLP,model,wmt_fr,Recurrent Continuous Translation Models,Nal Kalchbrenner|Phil Blunsom,Oxford,http://www.aclweb.org/anthology/D13-1176,CNN-based s2s translating improved with reranking.
2014,1412,NIPS,model,wmt_fr,Sequence to Sequence Learning with Neural Networks,Ilya Sutskever|Oriol Vinyals|Quoc V. Le,Google,https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks,RNN-based s2s model. 
2014,1410,EMNLP,model,wmt_fr,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,Kyunghyun Cho|Bart van Merrienboer|Caglar Gulcehre|Dzmitry BahdanauFethi Bougares|Holger Schwenk|Yoshua Bengio,Montreal|Jacobs|LeMans,http://www.aclweb.org/anthology/D14-1179,RNN-based enc-dec as features for smt. 
2014,1410,WORKSHOP,model|phrase,wmt_fr,Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation,Jean Pouget-Abadie|Dzmitry Bahdanau|Bart van Merrienboer|Kyunghyun Cho|Yoshua Bengio,Ecole-Polytechnique|Jacobs|Montreal,http://aclweb.org/anthology/W14-4009,Translating segments separately and concatenating them.
2014,1410,WORKSHOP,analysis,wmt_fr,On the Properties of Neural Machine Translation: Encoder-Decoder Approaches,Kyunghyun Cho|Bart van Merrienboer|Dzmitry Bahdanau|Yoshua Bengio,Montreal|Jacobs,http://www.aclweb.org/anthology/W14-4012,Analyzing the properties of NMT.
2015,1505,ICLR,model|attention,wmt_fr,Neural Machine Translation by Jointly Learning to Align and Translate,Dzmitry Bahdanau|Kyunghyun Cho|Yoshua Bengio,Jacobs|Montreal,https://arxiv.org/abs/1409.0473,Attention s2s.
2015,1507,ACL,vocab,wmt_fr|wmt_de,On Using Very Large Target Vocabulary for Neural Machine Translation,Sebastien Jean|Kyunghyun Cho|Roland Memisevic|Yoshua Bengio,Montreal,http://www.aclweb.org/anthology/P15-1001,Large traget vocab but only updating a subset of them when training.
2015,1507,ACL,vocab,wmt_fr,Addressing the Rare Word Problem in Neural Machine Translation,Minh-Thang Luong|Ilya Sutskever|Quoc V. Le|Oriol Vinyals|Wojciech Zaremba,Stanford|Google|NYU,http://www.aclweb.org/anthology/P15-1002,Augment UNK with position and alignment info.
2015,1509,EMNLP,vocab,wmt_fr,Variable-Length Word Encodings for Neural Translation Models,Rohan Chitnis|John DeNero,UCB,http://www.aclweb.org/anthology/D15-1249,Encode words with variable tokens.
2015,1509,EMNLP,model|attention,wmt_de,Effective Approaches to Attention-based Neural Machine Translation,Minh-Thang Luong|Hieu Pham|Christopher D.Manning,Stanford,http://aclweb.org/anthology/D15-1166,Different types of attention and input-feeding.
2015,1512,NIPS,train,other,Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,Samy Bengio|Oriol Vinyals|Navdeep Jaitly|Noam Shazeer,Google,https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks,Training with predicted y-feeding.
2016,1608,ACL,vocab,wmt_cs,Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models,Minh-Thang Luong|Christopher D.Manning,Stanford,http://www.aclweb.org/anthology/P16-1100,Hybrid model with char decoding for UNK.
2016,1608,ACL,vocab,wmt_fr|other,Pointing the Unknown Words,Caglar Gulcehre|Sungjin Ahn|Ramesh Nallapati|Bowen Zhou|Yoshua Bengio,Montreal|IBM,http://www.aclweb.org/anthology/P16-1014,As the title suggests.
2016,1608,ACL,syntax|attention,wat,Tree-to-Sequence Attentional Neural Machine Translation,Akiko Eriguchi|Kazuma Hashimoto|Yoshimasa Tsuruoka,Tokyo,http://www.aclweb.org/anthology/P16-1078,Source-side phrase tree.
2016,1608,ACL,semi,wmt_de|iwslt,Improving Neural Machine Translation Models with Monolingual Data,Rico Sennrich|Barry Haddow|Alexandra Birch,Edinburgh,http://www.aclweb.org/anthology/P16-1009,Synthetic source sentences with back translation.
2016,1608,ACL,model|smt,nist,Modeling Coverage for Neural Machine Translation,Zhaopeng Tu|Zhengdong Lu|Yang Liu|Xiaohua Liu|Hang Li,Huawei|Tsinghua,http://www.aclweb.org/anthology/P16-1008,Modeling coverage with coverage embedding for source words.
2016,1608,ACL,model|vocab,wmt_de|wmt_cs,A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation,Junyoung Chung|Kyunghyun Cho|Yoshua Bengio,Montreal|NYU,http://www.aclweb.org/anthology/P16-1160,Biscale decoder at char-level.
2016,1608,ACL,semi,nist,Semi-Supervised Learning for Neural Machine Translation,Yong Cheng|Wei Xu|Zhongjun He|Wei He|Hua Wu|Maosong Sun|Yang Liu,Tsinghua|Baidu,http://www.aclweb.org/anthology/P16-1185,With two-directions and auto-encoders.
2016,1608,ACL,train,nist|wmt_fr|wmt_de,Minimum Risk Training for Neural Machine Translation,Shiqi Shen|Yong Cheng|Zhongjun He|Wei He|Hua Wu|Maosong Sun|Yang Liu,Tsinghua|Baidu,http://www.aclweb.org/anthology/P16-1159,RL-like training with sampling.
2016,1608,ACL,vocab,wmt_de,Neural Machine Translation of Rare Words with Subword Units,Rico Sennrich|Barry Haddow|Alexandra Birch,Edinburgh,http://www.aclweb.org/anthology/P16-1162,BPE.
2016,1608,ACL,vocab,wmt_fr,Vocabulary Manipulation for Neural Machine Translation,Haitao Mi|Zhiguo Wang|Abe Ittycheriah,IBM,http://www.aclweb.org/anthology/P16-2021,Selected traget vocab.
2016,1608,ACL,syntax|smt,wmt_fr|wmt_de,Syntactically Guided Neural Machine Translation,Felix Stahlberg|Eva Hasler|Aurelien Waite|Bill Byrne,Cambridge,http://www.aclweb.org/anthology/P16-2049,Decoding with smt lattice.
2016,1608,ACL,vocab,wmt_de,Character-based Neural Machine Translation,Marta R. Costa-jussa|Jose A. R. Fonollosa,Catalunya,http://anthology.aclweb.org/P16-2058,Char-encoding for src.
2016,1608,ACL,vocab,other,Strategies for Training Large Vocabulary Neural Language Models,Wenlin Chen|David Grangier|Michael Auli,Facebook,http://www.aclweb.org/anthology/P16-1186,About target vocab.
2016,1611,EMNLP,multi,multi,Zero-Resource Translation with Multi-Lingual Neural Machine Translation,Orhan Firat|Baskaran Sankaran|Yaser Al-onaizan|Fatos T. Yarman Vural|Kyunghyun Cho,METU|IBM|NYU,http://aclweb.org/anthology/D16-1026,As the title suggests.
2016,1611,EMNLP,analysis,iwslt,Neural versus Phrase-Based Machine Translation Quality: a Case Study,Luisa Bentivogli|Arianna Bisazza|Mauro Cettolo|Marcello Federico,FBK|Amsterdam,http://aclweb.org/anthology/D16-1025,As the title suggests.
2016,1611,EMNLP,model,nist,Memory-enhanced Decoder for Neural Machine Translation,Mingxuan Wang|Zhengdong Lu|Hang Li|Qun Liu,ICT|Huawei|Dublin,https://aclweb.org/anthology/D16-1027,As the title suggests.
2016,1611,EMNLP,attention|smt,nist,Supervised Attentions for Neural Machine Translation,Haitao Mi|Zhiguo Wang|Abe Ittycheriah,IBM,https://aclweb.org/anthology/D16-1249,Additional alignment object.
2016,1611,EMNLP,model|train,nist,Variational Neural Machine Translation,Biao Zhang|Deyi Xiong|Jinsong Su|Hong Duan|Min Zhang,Soochow|Xiamen,http://aclweb.org/anthology/D16-1050,With a continuous latent variable.
2016,1611,EMNLP,analysis,wmt_fr,Why Neural Translations are the Right Length,Xing Shi|Kevin Knight|Deniz Yuret,USC|KU,https://aclweb.org/anthology/D16-1248,As the title suggests.
2016,1611,EMNLP,model|smt,nist,Coverage Embedding Models for Neural Machine Translation,Haitao Mi|Baskaran Sankaran|Zhiguo Wang|Abe Ittycheriah,IBM,https://aclweb.org/anthology/D16-1096,Coverage Embedding Models.
2016,1611,EMNLP,train,iwslt,Sequence-to-Sequence Learning as Beam-Search Optimization,Sam Wiseman|Alexander M. Rush,Harvard,https://aclweb.org/anthology/D16-1137,Global training with LaSO.
2016,1611,EMNLP,multi,multi,Transfer Learning for Low-Resource Neural Machine Translation,Barret Zoph|Deniz Yuret|Jonathan May|Kevin Knight,USC|KU,https://aclweb.org/anthology/D16-1163,As the title suggests.
2016,1611,EMNLP,train|prac,wmt_de,Sequence-Level Knowledge Distillation,Yoon Kim|Alexander M. Rush,Harvard,https://aclweb.org/anthology/D16-1139,Distillation to a smaller model.
2016,1611,EMNLP,vocab|smt,wat,Incorporating Discrete Translation Lexicons into Neural Machine Translation,Philip Arthur|Graham Neubig|Satoshi Nakamura,Naist|CMU,https://www.aclweb.org/anthology/D16-1162,Adding bias and linear interpolation.
2016,1611,EMNLP,semi,nist,Exploiting Source-side Monolingual Data in Neural Machine Translation,Jiajun Zhang|Chengqing Zong,CAS,http://aclweb.org/anthology/D16-1160,Self-learning and multi-task learning.
2016,1611,EMNLP,analysis|syntax,wmt_fr|wmt_de,Does String-Based Neural MT Learn Source Syntax,Xing Shi|Inkit Padhi|Kevin Knight,USC,http://www.aclweb.org/anthology/D16-1159,As the title suggests.
2016,1612,NIPS,train,wmt_fr,Dual Learning for Machine Translation,Di He|Yingce Xia|Tao Qin|Liwei Wang|Nenghai Yu|Tie-Yan Liu|Wei-Ying Ma,Peking|USTC|MSR,https://papers.nips.cc/paper/6469-dual-learning-for-machine-translation,Dual translation with RL.
2016,1612,NIPS,train,wmt_fr|other,Reward Augmented Maximum Likelihood for Neural Structured Prediction,Mohammad Norouzi|Samy Bengio|Zhifeng Chen|Navdeep Jaitly|Mike Schuster|Yonghui Wu|Dale Schuurmans,Google,https://papers.nips.cc/paper/6547-reward-augmented-maximum-likelihood-for-neural-structured-prediction,Training with loss-enhanced sampled instances.
2016,1602,AAAI,smt|vocab,nist,Improved Neural Machine Translation with SMT Features,Wei He|Zhongjun He|Hua Wu|HaifengWang,Baidu,https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12189,SMT plus NMT under log-linear framework.
2016,1612,COLING,model,nist,Topic-Informed Neural Machine Translation,Jian Zhang|Liangyou Li|Andy Way|Qun Liu,Dublin,http://aclweb.org/anthology/C16-1170,As the title suggests.
2016,1612,COLING,model|smt|attention,nist,Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation,Shi Feng|Shujie Liu|Nan Yang|Mu Li|Ming Zhou|Kenny Q. Zhu,UMD|MSR|SJTU,http://www.aclweb.org/anthology/C16-1290,As the title suggests.
2016,1612,COLING,attention|smt,nist,Neural Machine Translation with Supervised Attention,Lemao Liu|Masao Utiyama|Andrew Finch|Eiichiro Sumita,NICT,http://www.aclweb.org/anthology/C16-1291,Additional alignment object.
2016,1612,COLING,analysis,wmt_de,What Makes Word-level Neural Machine Translation Hard: A Case Study on English-German Translation,Fabian Hirschmann|Jinseok Nam|Johannes Furnkranz,Darmstadt,http://www.aclweb.org/anthology/C16-1301,As the title suggests.
2016,1612,COLING,smt|vocab,wmt_de,Pre-Translation for Neural Machine Translation,Jan Niehues|Eunah Cho|Thanh-Le Ha|Alex Waibel,Karlsruhe,http://www.aclweb.org/anthology/C16-1172,Hyp plus src to trg.
2016,1612,COLING,model|attention,nist,Interactive Attention for Neural Machine Translation,Fandong Meng|Zhengdong Lu|Hang Li|Qun Liu,Tencent|Huawei|Dublin|CAS,http://aclweb.org/anthology/C16-1205,Modeling src-side encodings in the decoding process.
2016,1607,IJCAI,vocab,nist,Towards Zero Unknown Word in Neural Machine Translation,Xiaoqing Li|Jiajun Zhang|Chengqing Zong,CAS,https://www.ijcai.org/Proceedings/16/Papers/405.pdf,Replacing RareWords with Similar words.
2016,1607,IJCAI,model|attention,nist|wmt_fr,Agreement-Based Joint Training for Bidirectional Attention-Based Neural Machine Translation,Yong Cheng|Shiqi Shen|Zhongjun He|Wei He|Hua Wu|Maosong Sun|Yang Liu,Tsinghua|Baidu,https://www.ijcai.org/Proceedings/16/Papers/392.pdf,Additional attention agreement object.
2016,1608,CoNLL,prac,wmt_de,Compression of Neural Machine Translation Models via Pruning,Abigail See|Minh-Thang Luong|Christopher D. Manning,Stanford,http://www.aclweb.org/anthology/K16-1029,Pruning to a smaller model.
2016,1606,NAACL,multi,multi,Multi-Way Multilingual Neural Machine Translation with a Shared Attention Mechanism,Orhan Firat|Kyunghyun Cho|Yoshua Bengio,METU|NYU|Montreal,http://www.aclweb.org/anthology/N16-1101,Shared attention.
2016,1606,NAACL,model|search,nist,Agreement on Target-bidirectional Neural Machine Translation,Lemao Liu|Masao Utiyama|Andrew Finch|Eiichiro Sumita,NICT,http://www.aclweb.org/anthology/N16-1046,Scoring with two-way.
2016,1606,NAACL,smt|attention,other,Incorporating Structural Alignment Biases into an Attentional Neural Translation Model,Trevor Cohn|Cong Duy Vu Hoang|Ekaterina Vymolova|Kaisheng Yao|Chris Dyer|Gholamreza Haffari,Melbourne|MSR|CMU|Monash,http://www.aclweb.org/anthology/N16-1102,As the title suggests.
2016,1606,NAACL,model,other,Controlling Politeness in Neural Machine Translation via Side Constraints,Rico Sennrich|Barry Haddow|Alexandra Birch,Edinburgh,http://www.aclweb.org/anthology/N16-1005,Sepcial token at the end of src.
2016,1606,NAACL,multi|attention,multi,Multi-Source Neural Translation,Barret Zoph|Kevin Knight,USC,http://www.aclweb.org/anthology/N16-1004,Multi-src attention.
2016,1607,TACL,model,wmt_fr|wmt_de,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,Jie Zhou|Ying Cao|Xuguang Wang|Peng Li|Wei Xu,Baidu,http://www.aclweb.org/anthology/Q16-1027,As the title suggests.
2016,1605,ICLR,train,iwslt|other,Sequence Level Training with Recurrent Neural Networks,Marc'Aurelio Ranzato|Sumit Chopra|Michael Auli|Wojciech Zaremba,Facebook,https://arxiv.org/abs/1511.06732,RL training.
2017,1704,EACL,model|attention,wmt_de|nist,Neural Machine Translation with Recurrent Attention Modeling,Zichao Yang|Zhiting Hu|Yuntian Deng|Chris Dyer|Alex Smola,CMU,http://www.aclweb.org/anthology/E17-2061,RNN for attention.
2017,1704,EACL,train,other,Learning to Translate in Real-time with Neural Machine Translation,Jiatao Gu|Graham Neubig|Kyunghyun Cho|Victor O.K. Li,HKU|CMU|NYU,http://www.aclweb.org/anthology/E17-1099,Learning actions with RL.
2017,1704,EACL,prac,other,Nematus: a Toolkit for Neural Machine Translation,Rico Sennrich|Orhan Firat|Kyunghyun Cho|Alexandra Birch|Barry Haddow|Julian Hitschler|Marcin Junczys-Dowmunt|Samuel Laubli|Antonio Valerio Miceli Barone|Jozef Mokry|Maria Nadejde,Edinburgh|METU|NYU|Heidelberg|Zurich,http://aclweb.org/anthology/E17-3017,Toolkit.
2017,1704,EACL,analysis,wmt_de|wmt_cs|other,A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions,Antonio Toral|Victor M.Sanchez-Cartagena,Groningen|Prompsit,http://aclweb.org/anthology/E17-1100,As the title suggests.
2017,1704,EACL,analysis,wmt_de,How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs,Rico Sennrich,Edinburgh,http://www.aclweb.org/anthology/E17-2060,As the title suggests.
2017,1704,EACL,train|smt,wmt_de|wat,Neural Machine Translation by Minimising the Bayes-risk with Respect to Syntactic Translation Lattices,Felix Stahlberg|Adria de Gispert|Eva Hasler|Bill Byrne,Cambridge,http://www.aclweb.org/anthology/E17-2058,As the title suggests.
2017,1707,ACL,syntax,nist,Modeling Source Syntax for Neural Machine Translation,Junhui Li|Deyi Xiong|Zhaopeng Tu|Muhua Zhu|Min Zhang|Guodong Zhou,Soochow|Tecent,http://aclweb.org/anthology/P17-1064,Source syntax as tags.
2017,1707,ACL,model,wmt_fr|wmt_de|nist,Deep Neural Machine Translation with Linear Associative Unit,Mingxuan Wang|Zhengdong Lu|Jie Zhou|Qun Liu,Tencent|DeeplyCurious|Baidu|CAS|Dublin,http://www.aclweb.org/anthology/P17-1013,As the title suggests.
2017,1707,ACL,syntax,nist|wat,Sequence-to-Dependency Neural Machine Translation,Shuangzhi Wu|Dongdong Zhang|Nan Yang|Mu Li|Ming Zhou,HIT|MSR,http://www.aclweb.org/anthology/P17-1065,Shift-reduce parsing plus NMT and joint learning.
2017,1707,ACL,vocab,wat,Neural Machine Translation via Binary Code Prediction,Yusuke Oda|Philip Arthur|Graham Neubig|Koichiro Yoshino|Satoshi Nakamura,Naist|CMU,https://www.aclweb.org/anthology/P17-1079,Encode target words.
2017,1707,ACL,analysis,iwslt,What do Neural Machine Translation Models Learn about Morphology,Yonatan Belinkov|Nadir Durrani|Fahim Dalvi|Hassan Sajjad|James Glass,Cambridge|HBKU,http://aclweb.org/anthology/P17-1080,As the title suggests.
2017,1707,ACL,train|smt,nist,Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization,Jiacheng Zhang|Yang Liu|Huanbo Luan|Jingfang Xu|Maosong Sun,Tsinghua|Sougo,http://aclweb.org/anthology/P17-1139,Additional object of KL.
2017,1707,ACL,analysis,nist,Visualizing and Understanding Neural Machine Translation,Yanzhuo Ding|Yang Liu|Huanbo Luan|Maosong Sun,Tsinghua,http://aclweb.org/anthology/P17-1106,Layer-wise Relevance Propagation.
2017,1707,ACL,model,nist,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,Jinchao Zhang|Mingxuan Wang|Qun Liu|Jie Zhou,CAS|Baidu|Dublin,http://www.aclweb.org/anthology/P17-1140,Distortion Models for attention-based NMT.
2017,1707,ACL,train,other,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning Julia,Julia Kreutzer|Artem Sokolov|Stefan Riezler,Heidelberg,http://www.aclweb.org/anthology/P17-1138,RL-like training.
2017,1707,ACL,search,other,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Chris Hokamp|Qun Liu,Dublin,http://www.aclweb.org/anthology/P17-1141,Grid beam search (for word constrains).
2017,1707,ACL,model|phrase,wat,Chunk-based Decoder for Neural Machine Translation,Shonosuke Ishiwatar|Jingtao Yao|Shujie Liu|Mu Li|Ming Zhou|Naoki Yoshinaga|Masaru Kitsuregawa|Weijia Jia,Tokyo|SJTU|MSR,http://www.aclweb.org/anthology/P17-1174,Two layers with different connections explicitly modeling chunks.
2017,1707,ACL,multi|attention,other,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,Iacer Calixto|Qun Liu|Nick Campbell,Dublin,http://www.aclweb.org/anthology/P17-1175,As the title suggests.
2017,1707,ACL,multi|train,other,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,Yun Chen|Yang Liu|Yong Cheng|Victor O.K. Li,HKU|Tsinghua,http://www.aclweb.org/anthology/P17-1176,As the title suggests.
2017,1707,ACL,model,wmt_fr|wmt_de,A Convolutional Encoder Model for Neural Machine Translation,Jonas Gehring|Michael Auli|David Grangier|Yann N. Dauphin,Facebook,http://www.aclweb.org/anthology/P17-1012,CNN Encoder.
2017,1707,ACL,syntax,nist,Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder,Huadong Chen|Shujian Huang|David Chiang|Jiajun Chen,NJU|NotreDame,http://www.aclweb.org/anthology/P17-1177,A bidirectional tree encoder and a tree-cov model.
2017,1707,ACL,syntax,wat,Learning to Parse and Translate Improves Neural Machine Translation,Akiko Eriguchi|Yoshimasa Tsuruoka|Kyunghyun Cho,Tokyo|NYU,http://www.aclweb.org/anthology/P17-2012,Joint dependency and NMT.
2017,1707,ACL,multi|attention,other,Attention Strategies for Multi-Source Sequence-to-Sequence Learning,Jindrich Libovicky|Jindrich Helcl,UFAL,http://www.aclweb.org/anthology/P17-2031,As the title suggests.
2017,1707,ACL,syntax,wmt_de,Towards String-to-Tree Neural Machine Translation,Roee Aharoni|Yoav Goldberg,Bar-Ilan,http://www.aclweb.org/anthology/P17-2021,String to a linearized and lexicalized constituency tree
2017,1707,ACL,model|multi,nist,Neural System Combination for Machine Translation,Long Zhou|Wenpeng Hu|Jiajun Zhang|Chengqing Zong,CAS,http://www.aclweb.org/anthology/P17-2060,Attention from multiple encoders.
2017,1707,ACL,semi|vocab,wmt_de,Data Augmentation for Low-Resource Neural Machine Translation,Marzieh Fadaee|Arianna Bisazza|Christof Monz,Amsterdam,http://www.aclweb.org/anthology/P17-2090,Data Augmentation by changing words.
2017,1707,ACL,model|phrase,nist,Chunk-Based Bi-Scale Decoder for Neural Machine Translation,Hao Zhou|Zhaopeng Tu|Shujian Huang|Xiaohua Liu|Hang Li|Jiajun Chen,NJU|Tecent|Huawei,http://www.aclweb.org/anthology/P17-2092,Two layers with different connections explicitly modeling chunks.
2017,1707,ACL,domain,wmt_fr|iwslt,Sentence Embedding for Neural Machine Translation Domain Adaptation,Rui Wang|Andrew Finch|Masao Utiyama|Eiichiro Sumita,NICT,http://www.aclweb.org/anthology/P17-2089,Selecting sentences for domain adaptation.
2017,1707,ACL,domain,wat|iwslt,An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation,Chenhui Chu|Raj Dabre|Sadao Kurohashi,Osaka|Kyoto,http://www.aclweb.org/anthology/P17-2061,As the title suggests.
2017,1707,ACL,prac|vocab,wat|wmt_fr,Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary,Xing Shi|Kevin Knight,USC,http://www.aclweb.org/anthology/P17-2091,As the title suggests.
2017,1707,ACL,prac,wmt_de,OpenNMT: Open-Source Toolkit for Neural Machine Translation,Guillaume Klein|Yoon Kim|Yuntian Deng|Jean Senellart|Alexander M. Rush,SYSTRAN|Harvard,http://www.aclweb.org/anthology/P17-4012,Toolkit.
2017,1707,ACL,train,iwslt,Differentiable Scheduled Sampling for Credit Assignment,Kartik Goyal|Chris Dyer|Taylor Berg-Kirkpatrick,CMU|Deepmind,http://www.aclweb.org/anthology/P17-2058,Training with alpha-soft argmax.
2017,1709,EMNLP,multi,multi,Incorporating Global Visual Features into Attention-Based Neural Machine Translation,Iacer Calixto|Qun Liu,Dublin,http://aclweb.org/anthology/D17-1106,As the title suggests.
2017,1709,EMNLP,model,other,Neural Lattice-to-Sequence Models for Uncertain Inputs,Matthias Sperber|Graham Neubig|Jan Niehues|AlexWaibel,Karlsruhe|CMU,http://aclweb.org/anthology/D17-1146,Encoding on the source lattice.
2017,1709,EMNLP,vocab|smt,nist,Memory-augmented Neural Machine Translation,Yang Feng|Shiyue Zhang|Andi Zhang|Dong Wang|Andrew Abel,Tsinghua|CAS|BUPT|XJTLU,http://aclweb.org/anthology/D17-1147,Additional memory and attention strucutre for external vocabs.
2017,1709,EMNLP,domain,wmt_de|iwslt,Dynamic Data Selection for Neural Machine Translation,Marlies van derWees|Arianna Bisazza|Christof Monz,Amsterdam|Leiden,http://aclweb.org/anthology/D17-1148,Vary the selected subset of training data between different training epochs.
2017,1709,EMNLP,phrase|smt,nist,Translating Phrases in Neural Machine Translation,Xing Wang|Zhaopeng Tu|Deyi Xiong|Min Zhang,Soochow|Tecent,http://aclweb.org/anthology/D17-1150,Combining SMT and NMT model.
2017,1709,EMNLP,phrase|smt,wmt_de,Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search,Leonard Dahlmann|Evgeny Matusov|Pavel Petrushkov|Shahram Khadivi,eBay,http://aclweb.org/anthology/D17-1149,Log linear combination and hybrid search.
2017,1709,EMNLP,model|syntax,nist,Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation,Baosong Yang|Derek F.Wong|Tong Xiao|Lidia S. Chao|Jingbo Zhu,Macau|Northeastern,http://aclweb.org/anthology/D17-1151,Bidirectional encoding on a phrase tree.
2017,1709,EMNLP,model|syntax,wat,Neural Machine Translation with Source-Side Latent Graph Parsing,Kazuma Hashimoto|Yoshimasa Tsuruoka,Tokyo,http://aclweb.org/anthology/D17-1012,Encoding with src modified(relaxed) dep-representations.
2017,1709,EMNLP,semi,wmt_de,Unsupervised Pretraining for Sequence to Sequence Learning,Prajit Ramachandran|Peter J. Liu|Quoc V. Le,Google,http://aclweb.org/anthology/D17-1039,Pretraining for LM.
2017,1709,EMNLP,model,nist|wmt_de,Neural Machine Translation with Word Predictions,Rongxiang Weng|Shujian Huang|Zaixiang Zheng|Xinyu Dai|Jiajun Chen,NJU,http://aclweb.org/anthology/D17-1013,Extra word predictions tasks help training.
2017,1709,EMNLP,train|search,wmt_de|iwslt,Towards Decoding as Continuous Optimisation in Neural Machine Translation,Cong Duy Vu Hoang|Gholamreza Haffari|Trevor Cohn,Melbourne|Monash,http://aclweb.org/anthology/D17-1014,As the title suggests no recomb-search of decoding.
2017,1709,EMNLP,analysis,other,A Causal Framework for Explaining the Predictions of Black-box Sequence-to-sequence Models,David Alvarez-Melis|Tommi S. Jaakkola,MIT,http://aclweb.org/anthology/D17-1042,Analyzing the relations of input-output.
2017,1709,EMNLP,multi,multi,An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,Jean-Benoit Delbrouck|Stephane Dupont,umons,http://aclweb.org/anthology/D17-1096,As the title suggests.
2017,1709,EMNLP,train,other,Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback,Khanh Nguyen|Hal Daume III|Jordan Boyd-Graber,UMD|MSR,http://aclweb.org/anthology/D17-1153,Training with only feedbacks and no ref.
2017,1709,EMNLP,train|prac,wmt_de|nist,Towards Compact and Fast Neural Machine Translation Using a Combined Method,Xiaowei Zhang|Wei Chen|Feng Wang|Shuang Xu|Bo Xu,CAS,http://aclweb.org/anthology/D17-1154,Speeding up and sequence editing as gold.
2017,1709,EMNLP,train|prac,iwslt,Regularization techniques for fine-tuning in neural machine translation,Antonio Valerio Miceli Barone|Barry Haddow|Ulrich Germann|Rico Sennrich,Edinburgh,http://aclweb.org/anthology/D17-1156,As the title suggests.
2017,1709,EMNLP,domain,wmt_fr|iwslt,InstanceWeighting for Neural Machine Translation Domain Adaptation,Rui Wang|Masao Utiyama|Lemao Liu|Kehai Chen|Eiichiro Sumita,NICT|Tecent|HIT,http://aclweb.org/anthology/D17-1155,As the title suggests.
2017,1709,EMNLP,train|search,wmt_de|wmt_cs,Trainable Greedy Decoding for Neural Machine Translation,Jiatao Gu|Kyunghyun Cho|Victor O.K. Li,HKU|NYU,http://aclweb.org/anthology/D17-1209,Stacking another one (trained with RL) for manipulating hidden layer.
2017,1709,EMNLP,prac,wat|wmt_de,Unfolding and Shrinking Neural Machine Translation Ensembles,Felix Stahlberg|Bill Byrne,Cambridge,http://aclweb.org/anthology/D17-1207,As the title suggests.
2017,1709,EMNLP,semi,wmt_fr|wmt_de|wmt_cs,Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning,Tobias Domhan|Felix Hieber,Amazon,http://aclweb.org/anthology/D17-1158,As the title suggests.
2017,1709,EMNLP,model|syntax,wmt_de|wmt_cs,Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,Joost Bastings|Ivan Titov|Wilker Aziz|Diego Marcheggiani|Khalil Sima'an,Amsterdam|Edinburgh,http://aclweb.org/anthology/D17-1208,Encoding with dependencies.
2017,1709,EMNLP,model|syntax,nist,Neural Machine Translation with Source Dependency Representation,Kehai Chen|Rui Wang|Masao Utiyama|Lemao Liu|Akihiro Tamura|Eiichiro Sumita|Tiejun Zhao,HIT|NICT|Tecent|Ehime,http://aclweb.org/anthology/D17-1303,Encoding with dependencies.
2017,1709,EMNLP,model,nist,Exploiting Cross-Sentence Context for Neural Machine Translation,Longyue Wang|Zhaopeng Tu|Andy Way|Qun Liu,Dublin|Tecent,http://aclweb.org/anthology/D17-1300,As the title suggests.
2017,1709,EMNLP,prac,wmt_fr,Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU,Jacob Devlin,MSR,http://aclweb.org/anthology/D17-1299,Speeding up.
2017,1709,EMNLP,search,nist,When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size),Liang Huang|Kai Zhao|Mingbo Ma,Oregon,http://aclweb.org/anthology/D17-1226,As the title suggests.
2017,1702,AAAI,model|search,nist,Neural Machine Translation with Reconstruction,Zhaopeng Tu|Yang Liu|Lifeng Shang|Xiaohua Liu|Hang Li,Huawei|Tsinghua,https://arxiv.org/abs/1611.01874,Reconstruction as auto-encoder.
2017,1702,AAAI,smt,nist,Neural Machine Translation Advised by Statistical Machine Translation,Xing Wang|Zhengdong Lu|Zhaopeng Tu|Hang Li|Deyi Xiong|Min Zhang,Soochow|Huawei,https://arxiv.org/abs/1610.05150,As the title suggests.
2017,1708,IJCAI,model|multi,nist,ME-MD: An Effective Framework for Neural Machine Translation with Multiple Encoders and Decoders,Jinchao Zhang|Qun Liu|Jie Zhou,CAS|Baidu|Dublin,https://www.ijcai.org/proceedings/2017/474,Stacking multiple models.
2017,1708,IJCAI,syntax,nist,Improved Neural Machine Translation with Source Syntax,Shuangzhi Wu|Ming Zhou|Dongdong Zhang,HIT|MSR,https://www.ijcai.org/proceedings/2017/584,Additional reordered seq by dep.
2017,1708,IJCAI,multi,multi,Maximum Expected Likelihood Estimation for Zero-Resource Neural Machine Translation,Hao Zheng|Yong Cheng|Yang Liu,Beihang|Tsinghua,https://www.ijcai.org/proceedings/2017/594,As the title suggests.
2017,1703,TACL,model,nist,Context Gates for Neural Machine Translation,Zhaopeng Tu|Yang Liu|Zhengdong Lu|Xiaohua Liu|Hang Li,Huawei|Tsinghua,http://www.aclweb.org/anthology/Q17-1007,Context gates for src and trg.
2017,1707,WORKSHOP,search,wmt_de,Beam Search Strategies for Neural Machine Translation,Markus Freitag|Yaser Al-Onaizan,IBM,http://www.aclweb.org/anthology/W17-3207,As the title suggests.
2017,1707,WORKSHOP,analysis,wmt_de,Six Challenges for Neural Machine Translation,Philipp Koehn|Rebecca Knowles,JHU,http://www.aclweb.org/anthology/W17-3204,As the title suggests.
2017,1700,ARXIV,prac,other,Neural Machine Translation and Sequence-to-sequence Models: A Tutorial,Graham Neubig,CMU,https://arxiv.org/abs/1703.01619,Tutorial.
2017,1707,WORKSHOP,prac,wat|wmt_de,An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation,Makoto Morishita|Yusuke Oda|Graham Neubig|Koichiro Yoshino|Katsuhito Sudoh|Satoshi Nakamura,NTT|Naist|CMU|PRESTO,http://www.aclweb.org/anthology/W17-3208,As the title suggests.
2017,1708,WORKSHOP,phrase|model,iwslt,Toward Neural Phrase-based Machine Translation,Po-Sen Huang|Chong Wang|Dengyong Zhou|Li Deng,MSR,https://deepstruct.github.io/ICML17/1stDeepStructWS_paper_7.pdf,As the title suggests.
2016,1600,ARXIV,model|prac,wmt_fr|wmt_de,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,Yonghui Wu|et al.,Google,https://arxiv.org/abs/1609.08144,Google's NMT.
2016,1600,ARXIV,model,wmt_de,Neural Machine Translation in Linear Time,Nal Kalchbrenner|Lasse Espeholt|Karen Simonyan|Aaron van den Oord|Alex Graves|Koray Kavukcuoglu,Deepmind,https://arxiv.org/abs/1610.10099,S2s with conv-net.
2017,1708,WORKSHOP,analysis|search,wmt_de|iwslt,Analyzing Neural MT Search and Model Performance,Jan Niehues|Eunah Cho|Thanh-Le Ha|Alex Waibel,Karlsruhe,http://www.aclweb.org/anthology/W17-3202,Search-error or model-error.
2017,1708,WORKSHOP,model|syntax,other,Towards Neural Machine Translation with Latent Tree Attention,James Bradbury|Richard Socher,salesforce,http://www.aclweb.org/anthology/W17-4303,As the title suggests.
2016,1600,ARXIV,search,wmt_de|other,A Simple Fast Diverse Decoding Algorithm for Neural Generation,Jiwei Li|Will Monroe|Dan Jurafsky,Stanford,https://arxiv.org/abs/1611.08562,Encouriging diversity.
2017,1700,NIPS,model,wmt_fr|wmt_de,Attention Is All You Need,Ashish Vaswani|Noam Shazeer|Niki Parmar|Jakob Uszkoreit|Llion Jones|Aidan N.Gomez|Lukasz Kaiser,Google|Toronto,https://arxiv.org/abs/1706.03762,No rec layers.
2017,1700,ARXIV,model|vocab,iwslt|other,Improving Lexical Choice in Neural Machine Translation,Toan Q.Nguyen|David Chiang,NotreDame,https://arxiv.org/abs/1710.01329,As the title suggests.
2017,1710,TACL,model|multi,wmt_de|wmt_fr,Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation,Melvin Johnson|et al.,Google,http://www.aclweb.org/anthology/Q17-1024,One model for all languages
2017,1700,ARXIV,train|prac,wmt_cs,Curriculum Learning and Minibatch Bucketing in Neural Machine Translation,Tom Kocmi|Ondrej Bojar,Charles,https://arxiv.org/abs/1707.09533,As the title suggests.
2017,1700,ARXIV,model,wmt_de,Deep Architectures for Neural Machine Translation,Antonio Valerio Miceli Barone|Jindrich Helcl|Rico Sennrich|Barry Haddow|Alexandra Birch,Edinburgh|Charles,https://arxiv.org/abs/1707.07631,As the title suggests.
2017,1700,ARXIV,search|train,other,A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models,Kartik Goyal|Graham Neubig|Chris Dyer|Taylor Berg-Kirkpatrick,CMU|Deepmind,https://arxiv.org/abs/1708.00111,As the title suggests.
2017,1700,ARXIV,train|search,iwslt,Learning to Decode for Future Success,Jiwei Li|Will Monroe|Dan Jurafsky,Stanford,https://arxiv.org/abs/1701.06549,The model can be thought of as a simple version of the actor-critic model.
2017,1708,WORKSHOP,prac,wmt_de|iwslt,Stronger Baselines for Trustable Results in Neural Machine Translation,Michael Denkowski|Graham Neubig,Amazon|CMU,http://www.aclweb.org/anthology/W17-3203,As the title suggests.
2016,1608,ACL,model,other,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,Jiatao Gu|Zhengdong Lu|Hang Li|Victor O.K. Li,HKU|Huawei,http://www.aclweb.org/anthology/P16-1154,Copying mechanism.
2016,1611,EMNLP,model|search,other,Controlling Output Length in Neural Encoder-Decoders,Yuta Kikuchi|Graham Neubig|Ryohei Sasano|Hiroya Takamura|Manabu Okumura,TIT|CMU,http://www.aclweb.org/anthology/D16-1140,As the title suggests.
2016,1611,EMNLP,model|train,other,Length bias in Encoder Decoder Models and a Case for Global Conditioning,Pavel Sountsov|Sunita Sarawagi,Google|IITB,http://www.aclweb.org/anthology/D16-1158,Globally conditioned enc-dec models.
2017,1709,EMNLP,model|attention|search,other,Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models,Louis Shao|Stephan Gouws|Denny Britz|Anna Goldie|Brian Strope|Ray Kurzweil,Google,http://www.aclweb.org/anthology/D17-1234,Self-attention to the decoder and seg-by-seg reranking.
